{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/athakur36/LLMs-and-Ego-Development/blob/main/LLM_Gemini_Bloom.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oKIQ9aHudleX"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow 2.x\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAiP6hLCdp4q"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avr9HYZaWDr_"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import BloomForCausalLM\n",
        "from transformers import BloomTokenizerFast\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kky5EujwMHRs"
      },
      "outputs": [],
      "source": [
        "temp_values=[0.0001,0.2,0.5,0.8, 0.9]\n",
        "max_lengths=[500]\n",
        "response_df = pd.DataFrame(columns=[\"Prompt\", \"Engine\", \"Response\", \"Temperature\", \"max_length\"])\n",
        "prompts = [\n",
        "    \"Raising a family\",\n",
        "    \"When I am criticized\",\n",
        "    \"Change is\",\n",
        "    \"Education\",\n",
        "    \"When people are helpless\",\n",
        "    \"The past\",\n",
        "    \"I just can’t stand people who\",\n",
        "    \"Rules\",\n",
        "    \"At times I worry about\",\n",
        "    \"Privacy\",\n",
        "    \"Sometimes I wish that\",\n",
        "    \"A good boss\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WGNyDp2eHv1"
      },
      "outputs": [],
      "source": [
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = TFGPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7TgIrcF2eMFM"
      },
      "outputs": [],
      "source": [
        "def generate_text(prompt, max_length=100, num_return_sequences=1):\n",
        "    # Encode the input prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='tf')\n",
        "\n",
        "    # Create the attention mask\n",
        "    attention_mask = tf.ones(input_ids.shape, dtype=tf.int32)\n",
        "\n",
        "    # Generate text with the GPT-2 model\n",
        "    output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=max_length,\n",
        "    num_return_sequences=3,\n",
        "    no_repeat_ngram_size=3,\n",
        "    pad_token_id=model.config.pad_token_id,\n",
        "    eos_token_id=model.config.eos_token_id,\n",
        "    early_stopping=True,\n",
        "    num_beams=5,\n",
        "    )\n",
        "\n",
        "    # Decode and return the generated text\n",
        "    generated_text = [\n",
        "        tokenizer.decode(output_sequence, skip_special_tokens=True)\n",
        "        for output_sequence in output\n",
        "    ]\n",
        "\n",
        "    return generated_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFD-Gk0leP9G"
      },
      "outputs": [],
      "source": [
        "prompt = \"When I am criticized\"\n",
        "generated_text = generate_text(prompt, max_length=200, num_return_sequences=3)\n",
        "\n",
        "for idx, text in enumerate(generated_text):\n",
        "    print(f\"Generated text {idx + 1}:\")\n",
        "    print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QF6vSNkKGdDW"
      },
      "source": [
        "Here openai-gpt is using first generation of GPT associated with https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf\n",
        "\n",
        "https://huggingface.co/openai-gpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyw59doROzad"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "hf_api_key= userdata.get('hugging_face_key')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8hjX2ODwbJx"
      },
      "outputs": [],
      "source": [
        "temp_values=[0.0001,0.2,0.3,0.5,0.7,0.8, 0.9, 1]\n",
        "response_df = pd.DataFrame(columns=[\"Prompt\", \"Engine\", \"Response\", \"Temperature\"])\n",
        "prompts = [\n",
        "    \"Complete the following sentence thoughtfully and honestly with your own words. There are no right or wrong response: Raising a family\",\n",
        "    \"Complete the following sentence thoughtfully and honestly with your own words. There are no right or wrong response: When I am criticized\",\n",
        "    \"Complete the following sentence thoughtfully and honestly with your own words. There are no right or wrong response: Change is\",\n",
        "    \"Complete the following sentence thoughtfully and honestly with your own words. There are no right or wrong response: Education\",\n",
        "    \"Complete the following sentence thoughtfully and honestly with your own words. There are no right or wrong response: When people are helpless\",\n",
        "    \"Complete the following sentence thoughtfully and honestly with your own words. There are no right or wrong response: The past\",\n",
        "    \"Complete the following sentence thoughtfully and honestly with your own words. There are no right or wrong response: I just can’t stand people who\",\n",
        "    \"Complete the following sentence thoughtfully and honestly with your own words. There are no right or wrong response: Rules\",\n",
        "    \"Complete the following sentence thoughtfully and honestly with your own words. There are no right or wrong response: At times I worry about\",\n",
        "    \"Complete the following sentence thoughtfully and honestly with your own words. There are no right or wrong response: Privacy\",\n",
        "    \"Complete the following sentence thoughtfully and honestly with your own words. There are no right or wrong response: Sometimes I wish that\",\n",
        "    \"Complete the following sentence thoughtfully and honestly with your own words. There are no right or wrong response: A good boss\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4-hLRwygkFh"
      },
      "outputs": [],
      "source": [
        "model_blm_560m = BloomForCausalLM.from_pretrained(\"bigscience/bloomz-560m\", token=hf_api_key)\n",
        "tokenizer_blm_560m = BloomTokenizerFast.from_pretrained(\"bigscience/bloomz-560m\", token=hf_api_key)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3r30e_jNzTV"
      },
      "outputs": [],
      "source": [
        "model_bloomz_1b1 = BloomForCausalLM.from_pretrained(\"bigscience/bloomz-1b1\", token=hf_api_key)\n",
        "tokenizer_bloomz_1b1 = BloomTokenizerFast.from_pretrained(\"bigscience/bloomz-1b1\", token=hf_api_key)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kyVsLZwGObM5"
      },
      "outputs": [],
      "source": [
        "tokenizer_gpt_j_6b = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6b\", token=hf_api_key)\n",
        "model_gpt_j_6b = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6b\", token=hf_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOcvfZbEROZm"
      },
      "outputs": [],
      "source": [
        "model_bloomz_3b = BloomForCausalLM.from_pretrained(\"bigscience/bloomz-3b\", token=hf_api_key)\n",
        "tokenizer_bloomz_3b = BloomTokenizerFast.from_pretrained(\"bigscience/bloomz-3b\", token=hf_api_key)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "g3u4iuKRMdxA"
      },
      "outputs": [],
      "source": [
        "def generate_responses(model, tokenizer, engine_name, max_lengths, temp_values, prompts, pad_token_id=None, attention_mask=None, sample= False):\n",
        "    response_df = pd.DataFrame(columns=[\"Prompt\", \"Engine\", \"Response\", \"Temperature\", \"max_length\"])\n",
        "\n",
        "    for max_length in max_lengths:\n",
        "        for temp in temp_values:\n",
        "            for prompt in prompts:\n",
        "                inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "                response = tokenizer.decode(model.generate(inputs[\"input_ids\"],\n",
        "                                                            max_length=max_length,\n",
        "                                                            pad_token_id=pad_token_id,  # Set pad token ID if provided\n",
        "                                                            attention_mask=attention_mask,  # Set attention mask if provided\n",
        "                                                            temperature=temp,\n",
        "                                                            do_sample=sample)[0])\n",
        "                response_df = response_df.append({\"Prompt\": prompt, \"Engine\": engine_name, \"Response\": response, \"Temperature\": temp, \"max_length\": max_length}, ignore_index=True)\n",
        "\n",
        "    return response_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5V74IFwMgEn"
      },
      "outputs": [],
      "source": [
        "# Define models and tokenizers\n",
        "models = {\n",
        "    \"gpt-j-6b\": (model_gpt_j_6b, tokenizer_gpt_j_6b),\n",
        "    #\"bloomz_560m\": (model_blm_560m, tokenizer_blm_560m),\n",
        "    #\"bloomz_1b1\": (model_bloomz_1b1, tokenizer_bloomz_1b1),\n",
        "    #\"bloomz_3b\": (model_bloomz_3b, tokenizer_bloomz_3b)\n",
        "}\n",
        "\n",
        "# Generate responses for each model\n",
        "for engine_name, (model, tokenizer) in models.items():\n",
        "    if engine_name == \"gpt-j-6b\":\n",
        "        response_df = generate_responses(model, tokenizer, engine_name, max_lengths, temp_values, prompts, tokenizer.eos_token_id, sample=True)\n",
        "    else:\n",
        "        response_df = generate_responses(model, tokenizer, engine_name, max_lengths, temp_values, prompts)\n",
        "    file_name = f\"{engine_name}_responses_v1.4.xlsx\"\n",
        "    response_df.to_excel(file_name, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2h1KRFe9wvo"
      },
      "outputs": [],
      "source": [
        "prompts = [\n",
        "    \"Raising a family\",\n",
        "    \"When I am criticized\",\n",
        "    \"Change is\",\n",
        "    \"Education\",\n",
        "    \"When people are helpless\",\n",
        "    \"The past\",\n",
        "    \"I just can’t stand people who\",\n",
        "    \"Rules\",\n",
        "    \"At times I worry about\",\n",
        "    \"Privacy\",\n",
        "    \"Sometimes I wish that\",\n",
        "    \"A good boss\"\n",
        "]\n",
        "temp_values=[0.0001]\n",
        "response_df = pd.DataFrame(columns=[\"Prompt\", \"Engine\", \"Response\", \"Temperature\", \"max_length\"])\n",
        "tokenizer_gpt_j_6b = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6b\", token=hf_api_key)\n",
        "model_gpt_j_6b = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6b\", token=hf_api_key)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcOoPf8vfBgG"
      },
      "outputs": [],
      "source": [
        "response_df = response_df[0:0]\n",
        "max_lengths= [200,500]\n",
        "for temp in temp_values:\n",
        "  for max_length in max_lengths:\n",
        "    for prompt in prompts:\n",
        "      inputs = tokenizer_gpt_j_6b.decode(prompt, return_tensors=\"pt\")\n",
        "      #print(\"success\")\n",
        "      response = tokenizer_gpt_j_6b.decode(model_gpt_j_6b.generate(inputs[\"input_ids\"],\n",
        "                          max_length=max_length,\n",
        "                          temperature= temp,\n",
        "                          do_sample= True\n",
        "                          )[0])\n",
        "      response_df = response_df.append({\"Prompt\": prompt, \"Engine\": \"model_gpt_j_6b\", \"Response\": response, \"Temperature\": temp, \"max_length\": max_length}, ignore_index=True)\n",
        "  # Save the DataFrame to an Excel file\n",
        "  file_name = f\"Falcon_180B_responses_v1.4{max_length}.xlsx\"\n",
        "  response_df.to_excel(file_name, index=False)\n",
        "  response_df = response_df[0:0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tqQQVYyhKNEm"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "temp_values=[0.0001, 0.2, 0.3, 0.5, 0.7, 0.8, 0.9, 1]\n",
        "response_df = pd.DataFrame(columns=[\"Prompt\", \"Engine\", \"Response\", \"Temperature\"])\n",
        "prompts = [\n",
        "    \"Raising a family\",\n",
        "    \"When I am criticized\",\n",
        "    \"Change is\",\n",
        "    \"Education\",\n",
        "    \"When people are helpless\",\n",
        "    \"The past\",\n",
        "    \"I just can’t stand people who\",\n",
        "    \"Rules\",\n",
        "    \"At times I worry about\",\n",
        "    \"Privacy\",\n",
        "    \"Sometimes I wish that\",\n",
        "    \"A good boss\"\n",
        "]\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/bigscience/bloom\"\n",
        "#API_URL = \"https://api-inference.huggingface.co/models/EleutherAI/gpt-neo-2.7B\"\n",
        "#API_URL = \"https://api-inference.huggingface.co/models/EleutherAI/polyglot-ko-3.8b\"\n",
        "#API_URL = \"https://api-inference.huggingface.co/models/EleutherAI/gpt-neo-2.7B\"\n",
        "#API_URL = \"https://api-inference.huggingface.co/models/bigscience/bloomz-1b1\"\n",
        "headers = {\"Authorization\": \"Bearer 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\"}\n",
        "\n",
        "def query(payload):\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "    return response.json()\n",
        "for temp in temp_values:\n",
        "  for prompt in prompts:\n",
        "    output = query({\n",
        "        \"inputs\": prompt,\n",
        "        \"parameters\": { \"temperature\": temp,\"max_new_tokens\": 800},\n",
        "    })\n",
        "    #print(temp)\n",
        "    print(output)\n",
        "    #response_df = response_df.append({\"Prompt\": prompt, \"Engine\": \"bloom\", \"Response\": output[0]['generated_text'], \"Temperature\": temp}, ignore_index=True)\n",
        "#response_df.to_excel(\"responses__bloomz-3b_token_800_v1.4.xlsx\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BId3jMXnLk9b"
      },
      "source": [
        "Accessing google Palm2 api:\n",
        "1. Follow the instructions mentioned in https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/tutorials/python_quickstart.ipynb#scrollTo=G-zBkueElVEO\n",
        "to set up the environment\n",
        "2. Use genai.generate_text instead of genai.generate_content to access pal2 models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1EerbHYBGDK",
        "outputId": "971a58c8-fb3a-4edd-f5eb-1ee1c08fe74f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"error\": {\n",
            "    \"code\": 400,\n",
            "    \"message\": \"API key not valid. Please pass a valid API key.\",\n",
            "    \"status\": \"INVALID_ARGUMENT\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\n",
            "        \"reason\": \"API_KEY_INVALID\",\n",
            "        \"domain\": \"googleapis.com\",\n",
            "        \"metadata\": {\n",
            "          \"service\": \"generativelanguage.googleapis.com\"\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# check quickly if the api key is working\n",
        "!curl \\\n",
        "  -H 'Content-Type: application/json' \\\n",
        "  -d '{\"contents\":[{\"parts\":[{\"text\":\"Write a story about a magic backpack\"}]}]}' \\\n",
        "  -X POST https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=GOOGLE_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoqP41dXCMzW",
        "outputId": "cdedf4f7-0b80-4a7c-a91e-254351e842cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/158.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnibcqfCCbdt"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import textwrap\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "from IPython.display import display\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9-j78TwCgo2"
      },
      "outputs": [],
      "source": [
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "eSRNOaMXDTk2",
        "outputId": "911d307e-9162-4561-8503-b35c9da5d7b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "models/gemini-1.0-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-pro\n",
            "models/gemini-pro-vision\n"
          ]
        }
      ],
      "source": [
        "for m in genai.list_models():\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    print(m.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPH-zLwNDwHf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "temp_values=[0.0001, 0.2, 0.5, 0.8]\n",
        "response_df = pd.DataFrame(columns=[\"Prompt\", \"Engine\", \"Response\", \"Temperature\"])\n",
        "prompts = [\n",
        "    \"Complete the following sentence: Raising a family\",\n",
        "    \"Complete the following sentence: When I am criticized\",\n",
        "    \"Complete the following sentence: Change is\",\n",
        "    \"Complete the following sentence: Education\",\n",
        "    \"Complete the following sentence: When people are helpless\",\n",
        "    \"Complete the following sentence: The past\",\n",
        "    \"Complete the following sentence: I just can’t stand people who\",\n",
        "    \"Complete the following sentence: Rules\",\n",
        "    \"Complete the following sentence: At times I worry about\",\n",
        "    \"Complete the following sentence: Privacy\",\n",
        "    \"Complete the following sentence: Sometimes I wish that\",\n",
        "    \"Complete the following sentence: A good boss\"\n",
        "]\n",
        "#model_id=\"models/text-bison-001\"\n",
        "model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "for temp in temp_values:\n",
        "  for prompt in prompts:\n",
        "    generation_config = genai.GenerationConfig(\n",
        "    stop_sequences = None,\n",
        "    temperature=temp,\n",
        "    max_output_tokens=200\n",
        "  )\n",
        "    try:\n",
        "      response = model.generate_content(\n",
        "      contents = prompt,\n",
        "      generation_config=generation_config,\n",
        "      stream=False,\n",
        "    )\n",
        "      response_text = response.text if response.parts else 'No valid response'\n",
        "      print(response_text)\n",
        "    except Exception as e:\n",
        "      response_text = f\"Error generating response: {str(e)}\"\n",
        "    response = model.generate_content(\n",
        "    contents = prompt,\n",
        "    generation_config=generation_config,\n",
        "    stream=False,\n",
        "  )\n",
        "    #response=genai.generate_text( #for Palm2\n",
        "    #    model=model_id,\n",
        "    #   prompt=prompt,\n",
        "    #    temperature=temp,\n",
        "    #    max_output_tokens=200,\n",
        "    #)\n",
        "    #print(response.result)\n",
        "    #print(response)\n",
        "    new_row = pd.DataFrame([{\n",
        "    \"Prompt\": prompt,\n",
        "    \"Engine\": 'gemini-1.5-flash',\n",
        "    \"Response\": response.text,\n",
        "    \"Temperature\": temp\n",
        "    }])\n",
        "    response_df = pd.concat([response_df, new_row], ignore_index=True)\n",
        "#print(response_df['Response'])\n",
        "response_df.to_excel(\"LLM_responses__gemini_pro_200tokens_v1.4.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1YihVX1xqeRr"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgxjaCVJqs5p"
      },
      "outputs": [],
      "source": [
        "!pip install anthropic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQJN2NudqkVt"
      },
      "outputs": [],
      "source": [
        "import anthropic\n",
        "temp_values=[0.0001]\n",
        "prompts = [\n",
        "    \"Complete the following sentence: Raising a family\",\n",
        "    \"Complete the following sentence: When I am criticized\",\n",
        "    \"Complete the following sentence: Change is\",\n",
        "    \"Complete the following sentence: Education\",\n",
        "    \"Complete the following sentence: When people are helpless\",\n",
        "    \"Complete the following sentence: The past\",\n",
        "    \"Complete the following sentence: I just can’t stand people who\",\n",
        "    \"Complete the following sentence: Rules\",\n",
        "    \"Complete the following sentence: At times I worry about\",\n",
        "    \"Complete the following sentence: Privacy\",\n",
        "    \"Complete the following sentence: Sometimes I wish that\",\n",
        "    \"Complete the following sentence: A good boss\"\n",
        "]\n",
        "\n",
        "client = anthropic.Anthropic(\n",
        "    # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n",
        "    api_key=userdata.get('anthropic_key'),\n",
        ")\n",
        "for temp in temp_values:\n",
        "  for prompt in prompts:\n",
        "    message = client.messages.create(\n",
        "        model=\"claude-3-opus-20240229\",\n",
        "        max_tokens=200,\n",
        "        temperature=temp,\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": prompt\n",
        "                }\n",
        "            ]\n",
        "        }]\n",
        "    )\n",
        "    print(message.content[0].text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install mistralai"
      ],
      "metadata": {
        "id": "fIeU5HBqFYqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://colab.research.google.com/github/mistralai/cookbook/blob/main/quickstart.ipynb#scrollTo=e0eb939e-a7e6-42d9-a7ce-c61444c5dc62\n",
        "# code reference is taken from above codebook\n",
        "from mistralai.client import MistralClient\n",
        "from mistralai.models.chat_completion import ChatMessage\n",
        "temp_values=[0.0001]\n",
        "prompts = [\n",
        "    \"Complete the following sentence: Raising a family\",\n",
        "    \"Complete the following sentence: When I am criticized\",\n",
        "    \"Complete the following sentence: Change is\",\n",
        "    \"Complete the following sentence: Education\",\n",
        "    \"Complete the following sentence: When people are helpless\",\n",
        "    \"Complete the following sentence: The past\",\n",
        "    \"Complete the following sentence: I just can’t stand people who\",\n",
        "    \"Complete the following sentence: Rules\",\n",
        "    \"Complete the following sentence: At times I worry about\",\n",
        "    \"Complete the following sentence: Privacy\",\n",
        "    \"Complete the following sentence: Sometimes I wish that\",\n",
        "    \"Complete the following sentence: A good boss\"\n",
        "]\n",
        "api_key = userdata.get('mistral_key')\n",
        "model = \"mistral-large-latest\"\n",
        "client = MistralClient(api_key=api_key)\n",
        "\n",
        "for temp in temp_values:\n",
        "  for prompt in prompts:\n",
        "    chat_response = client.chat(\n",
        "    model=model,\n",
        "    messages=[ChatMessage(role=\"user\", content=prompt, temperature=temp, max_new_tokens=100)],\n",
        "    temperature=temp,\n",
        "    )\n",
        "\n",
        "    print(chat_response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "qFHb18v2FaZc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMCvDxd7e3a0hFjARfM8ohE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}