{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/athakur36/LLMs-and-Ego-Development/blob/main/LLMs_ego_dev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEhOev34w29-"
      },
      "source": [
        "## Introduction and API keys\n",
        "\n",
        "This notebook is taken from the following preprint: https://arxiv.org/abs/2302.02083\n",
        "\n",
        "We have modified the code to suit our own study. The original code can be found in: https://colab.research.google.com/drive/1ZRtmw87CdA4xp24DNS_Ik_uA2ypaRnoU?usp=sharing\n",
        "\n",
        "Please let me know if you spot any issues or can think of any improvements.\n",
        "\n",
        "To run this code, you will need to get:\n",
        "1. Your own API key from OpenAI account settings (https://platform.openai.com/account/api-keys)\n",
        "2. Your own API key from HuggingFace account settings (https://huggingface.co/docs/huggingface_hub/how-to-inference)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnxX_TbesXrG"
      },
      "source": [
        "##Set up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJ1vh9jf2WAg"
      },
      "outputs": [],
      "source": [
        "!pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NlYkMUski7u"
      },
      "outputs": [],
      "source": [
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "b09VLvUevl2s"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "import pandas as pd\n",
        "import requests\n",
        "from google.colab import userdata\n",
        "\n",
        "openai_key = userdata.get('openai_key')\n",
        "hugging_face_key = userdata.get('hugging_face_key')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KWopU6xMJ7AC"
      },
      "outputs": [],
      "source": [
        "#### Function for quering huggingface API for different versions of GPT-2\n",
        "def hf(prompt, engine=\"gpt2\", temperature=.01,max_tokens=50, top_p=0.95):\n",
        "    query ={\"inputs\": prompt,\n",
        "            \"parameters\": {\"temperature\": temperature,\"do_sample\":True,\"top_p\": top_p,\n",
        "             \"max_new_tokens\": max_tokens, \"max_time\": 120},\n",
        "            \"options\":{\"wait_for_model\":True}\n",
        "            }\n",
        "    API_URL = \"https://api-inference.huggingface.co/models/\"+engine\n",
        "    headers = {\"Authorization\": \"Bearer \"+hugging_face_key}\n",
        "    response = requests.post(API_URL, headers=headers, json=query)\n",
        "    print(temperature)\n",
        "    print(response.json())\n",
        "\n",
        "    out = response.json()\n",
        "    try:\n",
        "        out = out[0][\"generated_text\"][len(prompt):]\n",
        "    except KeyError:\n",
        "        # Handle the KeyError by returning a descriptive message or handling the error\n",
        "        out = \"Error in API response\"\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8tA40ipkjkQ9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to generate responses using the OpenAI Completion API\n",
        "def generate_response(prompt, max_tokens=50, engine=\"davinci\", temperature= 0.02):\n",
        "    \"\"\"\n",
        "    Generates a response based on the provided prompt.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The input prompt for text generation.\n",
        "        max_tokens (int): The maximum number of tokens in the generated response.\n",
        "        engine (str): The OpenAI engine to use for text generation.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated text response.\n",
        "    \"\"\"\n",
        "    print(temperature)\n",
        "    if engine in [\"gpt-4\", \"gpt-3.5-turbo\", \"gpt-4-1106-preview\"]:\n",
        "        completion = openai.ChatCompletion.create(\n",
        "            model=engine,\n",
        "            temperature= temperature,\n",
        "            messages=[{\"role\": \"system\", \"content\": \"Complete the following sentences:\"}, {\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=max_tokens\n",
        "        )\n",
        "        response = completion.choices[0].message[\"content\"].strip()\n",
        "    else:\n",
        "        completion = openai.Completion.create(\n",
        "            engine=engine,\n",
        "            temperature= temperature,\n",
        "            prompt=prompt,\n",
        "            max_tokens=max_tokens\n",
        "        )\n",
        "        response = completion.choices[0].text.strip()\n",
        "\n",
        "    return response\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTTwX07fmOEe"
      },
      "outputs": [],
      "source": [
        "# checking the list of available models by openai\n",
        "openai_api_key = openai_key\n",
        "\n",
        "url = \"https://api.openai.com/v1/models\"\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {openai_api_key}\"\n",
        "}\n",
        "response = requests.get(url, headers=headers)\n",
        "if response.status_code == 200:\n",
        "    # If the request was successful\n",
        "    models = response.json()\n",
        "    # Extracting and printing only the 'id' of each model\n",
        "    for model in models['data']:\n",
        "        print(model['id'])\n",
        "else:\n",
        "    # If there was an error\n",
        "    print(\"Error in API request:\", response.status_code, response.text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    \"Raising a family\",\n",
        "    \"When I am criticized\",\n",
        "    \"Change is\",\n",
        "    \"Education\",\n",
        "    \"When people are helpless\",\n",
        "    \"The past\",\n",
        "    \"I just can’t stand people who\",\n",
        "    \"Rules\",\n",
        "    \"At times I worry about\",\n",
        "    \"Privacy\",\n",
        "    \"Sometimes I wish that\",\n",
        "    \"A good boss\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "eL3M-Czw30e9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyCZwZWNjncl"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define a list of prompts and a list of engines\n",
        "temp_values=[0.0001]\n",
        "max_lengths = [50]\n",
        "data= []\n",
        "#response_df = pd.DataFrame(columns=[\"Prompt\", \"Engine\", \"Response\", \"Temperature\", \"max_length\"])\n",
        "# supported engines list can be found at https://platform.openai.com/docs/guides/gpt\n",
        "#engines_openai = [\"babbage-002\", \"text-ada-001\", \"ada\", \"text-babbage-001\", \"babbage\", \"curie\", \"text-curie-001\", \"davinci-002\", \"davinci\",\"gpt-4\", \"gpt-3.5-turbo\", \"gpt-4-1106-preview\", \"gpt-3.5-turbo-instruct\"]\n",
        "engines_openai = [\"babbage-002\",  \"davinci-002\", \"gpt-4\", \"gpt-3.5-turbo\", \"gpt-4-1106-preview\", \"gpt-3.5-turbo-instruct\"]\n",
        "\n",
        "\n",
        "# Loop through prompts and engines to generate responses\n",
        "for temp in temp_values:\n",
        "  for max_length in max_lengths:\n",
        "    for prompt in prompts:\n",
        "      for engine in engines_openai:\n",
        "      # print(engine)\n",
        "        response = generate_response(prompt, max_tokens=max_length, engine=engine, temperature= temp)\n",
        "        data.append({\"Prompt\": prompt, \"Engine\": engine, \"Response\": response, \"Temperature\": temp, \"max_length\": max_length})\n",
        "response_df= pd.DataFrame(data)\n",
        "file_name = f\"GPT_responses_len_v1.4.xlsx\"\n",
        "response_df.to_excel(file_name, index=False)\n",
        "response_df = response_df[0:0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bNI_7T4qe96"
      },
      "outputs": [],
      "source": [
        "temp_values=[0.2]\n",
        "max_lengths = [50]\n",
        "data=[]\n",
        "engines_hf = [\"gpt2-medium\", \"gpt2\", \"gpt2-large\", \"gpt2-xl\"]\n",
        "for temp in temp_values:\n",
        "  for max_length in max_lengths:\n",
        "    for prompt in prompts:\n",
        "      for engine in engines_hf:\n",
        "        response = hf(prompt, engine, temp, max_length)\n",
        "        data.append({\"Prompt\": prompt, \"Engine\": engine, \"Response\": response, \"Temperature\": temp, \"max_length\": max_length})\n",
        "    # Save the DataFrame to an Excel file\n",
        "response_df = pd.DataFrame(data)\n",
        "file_name = f\"GPT2_hf_responses_v1.4.xlsx\"\n",
        "response_df.to_excel(file_name, index=False)\n",
        "response_df = response_df[0:0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSUMv1oZGkbY"
      },
      "outputs": [],
      "source": [
        "# accessing EleutherAI/gpt-neo-125M, EleutherAI/gpt-neo-2.7B, AI21Labs/jurassic-1-jumbo\n",
        "from transformers import pipeline\n",
        "generator = pipeline('text-generation', model='EleutherAI/gpt-j-6b')\n",
        "generator(\"Raising a family\", do_sample=True, max_length=50, temperature= 0.7)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Z0F-240_bC3",
        "outputId": "1ce85373-86d9-4787-f8c8-389ee8d83c3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcUwSFQQ0IJ8"
      },
      "outputs": [],
      "source": [
        "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuB0MMlisc5d"
      },
      "outputs": [],
      "source": [
        "model_name = \"EleutherAI/gpt-neo-2.7B\"\n",
        "model = GPTNeoForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set the pad token to be the same as the eos token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "prompt = \"Raising a family\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=50)\n",
        "\n",
        "# Create an attention mask for the inputs\n",
        "attention_mask = inputs.attention_mask\n",
        "\n",
        "# Set pad_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "output_sequences = model.generate(\n",
        "    input_ids=inputs[\"input_ids\"],\n",
        "    attention_mask=attention_mask,\n",
        "    max_length=50,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=1\n",
        ")\n",
        "\n",
        "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfmOv8D3CJN1"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "temp_values=[0.0001]\n",
        "max_lengths = [200, 500]\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/EleutherAI/polyglot-ko-3.8b\"\n",
        "headers = {\"Authorization\": f\"Bearer {hugging_face_key}\"}\n",
        "\n",
        "def query(payload):\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "    return response.json()\n",
        "for max_length in max_lengths:\n",
        "  for temp in temp_values:\n",
        "    for prompt in prompts:\n",
        "      output = query({\n",
        "          \"inputs\": prompt,\n",
        "          \"parameters\": { \"temperature\": temp,\"max_new_tokens\": max_length},\n",
        "      })\n",
        "      #print(temp)\n",
        "      print(output)\n",
        "      response_df = response_df.append({\"Prompt\": prompt, \"Engine\": \"bloom\", \"Response\": output[0]['generated_text'], \"Temperature\": temp, \"max_length\": max_length}, ignore_index=True)\n",
        "file_name = f\"GPT_responses__polyglot_len_{max_length}_v1.4.xlsx\"\n",
        "response_df.to_excel(file_name, index=False)\n",
        "response_df = response_df[0:0]  # Clear the DataFrame for the next max_length iteration"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "HEhOev34w29-",
        "vnxX_TbesXrG",
        "y7gy3PwpsPlZ",
        "x02XyJ4bsirs",
        "8ycktQQWKtEM"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}